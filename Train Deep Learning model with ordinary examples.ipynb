{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model with dynamic Between - Class examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this Notebook x_train examples shuffle and combine efery second epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adamax\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "bands = 75\n",
    "frames = 75\n",
    "num_channels = 3\n",
    "classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **do_scale (x4d, scaler_list)**\n",
    " scales channels of samples with list of sklearn **StandartScalers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_scale(x4d, scaler_list):\n",
    "    (n_clips, n_time, n_freq, n_channel) = x4d.shape\n",
    "    x4d_scaled = np.zeros(x4d.shape)\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        x2d = x4d[:,:,:,channel].reshape((n_clips * n_time, n_freq))\n",
    "        x2d_scaled = scaler_list[channel].transform(x2d)\n",
    "        x3d_scaled = x2d_scaled.reshape((n_clips, n_time, n_freq))\n",
    "        x4d_scaled[:,:,:,channel] = x3d_scaled\n",
    "\n",
    "    return x4d_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **preprocess_data(x_train_path, y_train_path,x_test_path, y_test_path)**:\n",
    "1. Load data from .npy\n",
    "2. Fit Scalers on train data\n",
    "3. Remove unknown samples from test\n",
    "4. Transform y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x_train_path, y_train_path,x_test_path, y_test_path):\n",
    "    x_train = np.load(x_train_path).astype(np.float32)\n",
    "    y_train = np.load(y_train_path).astype(np.float32)\n",
    "    x_test = np.load(x_test_path).astype(np.float32)\n",
    "    y_test = np.load(y_test_path).astype(np.float32)\n",
    "\n",
    "    scaler_list = []\n",
    "    (n_clips, n_time, n_freq, n_channel) = x_train.shape\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        xtrain_2d = x_train[:, :, :, channel].reshape((n_clips * n_time, n_freq))\n",
    "        scaler = sklearn.preprocessing.StandardScaler().fit(xtrain_2d)\n",
    "        scaler_list += [scaler]\n",
    "\n",
    "    x_train = do_scale(x_train, scaler_list)\n",
    "    np.save('test_scaled.npy',do_scale(x_test, scaler_list))\n",
    "    x_test = do_scale(x_test, scaler_list)[:y_test.shape[0]-137, :, :, :]\n",
    "    y_train = keras.utils.to_categorical(y_train,classes)\n",
    "    y_test = y_test[:y_test.shape[0]-137]\n",
    "    y_test = keras.utils.to_categorical(y_test,classes)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # section 1\n",
    "\n",
    "    model.add(Convolution2D(filters=32, kernel_size=5,\n",
    "                            strides=2,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\",\n",
    "                            input_shape=(frames, bands, num_channels)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=32, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 2\n",
    "    model.add(Convolution2D(filters=64, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=64, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 3\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 4\n",
    "    model.add(Convolution2D(filters=512, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(filters=512, kernel_size=1,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # section 5\n",
    "    model.add(Convolution2D(filters=8, kernel_size=1,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **shuffle_in_unison(a, b)** mix x with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **combine_samples(x1,y1,x2,y2)** combine 2 samples with random ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_samples(x1,y1,x2,y2):\n",
    "    combine_k = np.random.random_sample()\n",
    "    x_new = combine_k*x1+(1-combine_k)*x2\n",
    "    y_new = combine_k*y1+(1-combine_k)*y2\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = preprocess_data('train_features.npy',\n",
    "                                                   'train_labels.npy',\n",
    "                                                   'test_features.npy',\n",
    "                                                   'test_labels.npy')\n",
    "\n",
    "x_train, y_train = shuffle_in_unison(x_train, y_train)\n",
    "\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-7)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "callbacks = [reduce_lr_on_plateau, early_stopping]\n",
    "acc_list = []\n",
    "\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adamax(0.01))\n",
    "json_model = json.loads(model.to_json())\n",
    "with open('model.json', 'w') as outfile:\n",
    "    json.dump(json_model, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=1)\n",
    "model.save_weights('weights/regular/model_weights1.hdf5')\n",
    "history2 = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=1)\n",
    "model.save_weights('weights/regular/model_weights2.hdf5')\n",
    "history3 = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=20)\n",
    "model.save_weights('weights/regular/model_weights20.hdf5')\n",
    "history4 = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=10)\n",
    "model.save_weights('weights/regular/model_weights30.hdf5')\n",
    "history5 = model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=10)\n",
    "model.save_weights('weights/regular/model_weights40.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately my machine does not allow me to fit the model fast, so the fitting is not complete and a model with a smaller val_loss is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4281749, 0.92389005]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights/regular/model_weights40.hdf5')\n",
    "print(model.test_on_batch(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not understand, how SQ metric works, so i haven't tested this metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
