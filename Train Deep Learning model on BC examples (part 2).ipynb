{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model with dynamic Between - Class examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this Notebook x_train examples shuffle and combine efery second epoch \n",
    "It helps the model not to overfit ( in some epochs val_loss < train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adamax\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "bands = 75\n",
    "frames = 75\n",
    "num_channels = 3\n",
    "classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **do_scale (x4d, scaler_list)**\n",
    " scales channels of samples with list of sklearn **StandartScalers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_scale(x4d, scaler_list):\n",
    "    (n_clips, n_time, n_freq, n_channel) = x4d.shape\n",
    "    x4d_scaled = np.zeros(x4d.shape)\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        x2d = x4d[:,:,:,channel].reshape((n_clips * n_time, n_freq))\n",
    "        x2d_scaled = scaler_list[channel].transform(x2d)\n",
    "        x3d_scaled = x2d_scaled.reshape((n_clips, n_time, n_freq))\n",
    "        x4d_scaled[:,:,:,channel] = x3d_scaled\n",
    "\n",
    "    return x4d_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **preprocess_data(x_train_path, y_train_path,x_test_path, y_test_path)**:\n",
    "1. Load data from .npy\n",
    "2. Fit Scalers on train data\n",
    "3. Remove unknown samples from test\n",
    "4. Transform y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x_train_path, y_train_path,x_test_path, y_test_path):\n",
    "    x_train = np.load(x_train_path).astype(np.float32)\n",
    "    y_train = np.load(y_train_path).astype(np.float32)\n",
    "    x_test = np.load(x_test_path).astype(np.float32)\n",
    "    y_test = np.load(y_test_path).astype(np.float32)\n",
    "\n",
    "    scaler_list = []\n",
    "    (n_clips, n_time, n_freq, n_channel) = x_train.shape\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        xtrain_2d = x_train[:, :, :, channel].reshape((n_clips * n_time, n_freq))\n",
    "        scaler = sklearn.preprocessing.StandardScaler().fit(xtrain_2d)\n",
    "        scaler_list += [scaler]\n",
    "\n",
    "    x_train = do_scale(x_train, scaler_list)\n",
    "    x_test = do_scale(x_test, scaler_list)[:y_test.shape[0]-137, :, :, :]\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train,classes)\n",
    "    y_test = y_test[:y_test.shape[0]-137]\n",
    "    y_test = keras.utils.to_categorical(y_test,classes)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # section 1\n",
    "\n",
    "    model.add(Convolution2D(filters=32, kernel_size=5,\n",
    "                            strides=2,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\",\n",
    "                            input_shape=(frames, bands, num_channels)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=32, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 2\n",
    "    model.add(Convolution2D(filters=64, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=64, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 3\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 4\n",
    "    model.add(Convolution2D(filters=512, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(filters=512, kernel_size=1,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # section 5\n",
    "    model.add(Convolution2D(filters=8, kernel_size=1,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **shuffle_in_unison(a, b)** mix x with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **combine_samples(x1,y1,x2,y2)** combine 2 samples with random ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_samples(x1,y1,x2,y2):\n",
    "    combine_k = np.random.random_sample()\n",
    "    x_new = combine_k*x1+(1-combine_k)*x2\n",
    "    y_new = combine_k*y1+(1-combine_k)*y2\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = preprocess_data('train_features.npy',\n",
    "                                                   'train_labels.npy',\n",
    "                                                   'test_features.npy',\n",
    "                                                   'test_labels.npy')\n",
    "x_shape = x_train.shape\n",
    "x_comb = np.zeros([int(x_shape[0]/2),x_shape[1],x_shape[2], x_shape[3]])\n",
    "y_shape = y_train.shape\n",
    "y_comb = np.zeros([int(y_shape[0]/2),y_shape[1]])\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    './weights/BC_d/BC_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5',\n",
    "    monitor='val_loss', save_best_only=True)\n",
    "\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-7)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "\n",
    "callbacks = [model_checkpoint, reduce_lr_on_plateau, early_stopping]\n",
    "\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adamax(0.01))\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 267s 47ms/step - loss: 2.4022 - acc: 0.2399 - val_loss: 2.6142 - val_acc: 0.1543\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 233s 41ms/step - loss: 2.1751 - acc: 0.3961 - val_loss: 2.5941 - val_acc: 0.0381\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 228s 40ms/step - loss: 1.9864 - acc: 0.4882 - val_loss: 2.4541 - val_acc: 0.1734\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 248s 44ms/step - loss: 1.8682 - acc: 0.5433 - val_loss: 2.2336 - val_acc: 0.3340\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 235s 42ms/step - loss: 1.7808 - acc: 0.5776 - val_loss: 2.2101 - val_acc: 0.3256\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 239s 42ms/step - loss: 1.6951 - acc: 0.6174 - val_loss: 2.2950 - val_acc: 0.3214\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 226s 40ms/step - loss: 1.6571 - acc: 0.6262 - val_loss: 2.2967 - val_acc: 0.2643\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 224s 40ms/step - loss: 1.5982 - acc: 0.6543 - val_loss: 2.2535 - val_acc: 0.3256\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 223s 40ms/step - loss: 1.5633 - acc: 0.6568 - val_loss: 2.3185 - val_acc: 0.2452\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 218s 39ms/step - loss: 1.5332 - acc: 0.6731 - val_loss: 2.1493 - val_acc: 0.3277\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.5083 - acc: 0.6800 - val_loss: 1.9919 - val_acc: 0.4228\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.4811 - acc: 0.6825 - val_loss: 2.0548 - val_acc: 0.4059\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 218s 39ms/step - loss: 1.4458 - acc: 0.6968 - val_loss: 2.0412 - val_acc: 0.4207\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.4228 - acc: 0.6915 - val_loss: 2.0586 - val_acc: 0.4101\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.3957 - acc: 0.7140 - val_loss: 2.0704 - val_acc: 0.3932\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.3660 - acc: 0.7196 - val_loss: 1.9832 - val_acc: 0.4313\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 218s 39ms/step - loss: 1.3619 - acc: 0.7196 - val_loss: 2.1015 - val_acc: 0.3784\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.3381 - acc: 0.7209 - val_loss: 1.8262 - val_acc: 0.4271\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.3268 - acc: 0.7306 - val_loss: 1.7767 - val_acc: 0.4376\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.2982 - acc: 0.7375 - val_loss: 2.0393 - val_acc: 0.3953\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.2878 - acc: 0.7504 - val_loss: 1.5439 - val_acc: 0.4968\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.2651 - acc: 0.7578 - val_loss: 1.7264 - val_acc: 0.4397\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.2512 - acc: 0.7619 - val_loss: 1.5336 - val_acc: 0.5391\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.2432 - acc: 0.7589 - val_loss: 1.8657 - val_acc: 0.4947\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.2255 - acc: 0.7750 - val_loss: 1.3248 - val_acc: 0.6342\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.2105 - acc: 0.7707 - val_loss: 1.3062 - val_acc: 0.6427\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.2030 - acc: 0.7833 - val_loss: 1.3775 - val_acc: 0.5899\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.1825 - acc: 0.7824 - val_loss: 1.3853 - val_acc: 0.5920\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.1809 - acc: 0.7883 - val_loss: 1.0122 - val_acc: 0.8034\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 222s 39ms/step - loss: 1.1574 - acc: 0.7960 - val_loss: 0.9822 - val_acc: 0.7907\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.1652 - acc: 0.7980 - val_loss: 1.4624 - val_acc: 0.5899\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.1750 - acc: 0.7796 - val_loss: 1.1990 - val_acc: 0.6575\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 218s 39ms/step - loss: 1.1229 - acc: 0.7934 - val_loss: 0.9320 - val_acc: 0.7632\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 219s 39ms/step - loss: 1.0992 - acc: 0.8074 - val_loss: 1.0176 - val_acc: 0.7696\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 220s 39ms/step - loss: 1.1346 - acc: 0.7964 - val_loss: 1.4054 - val_acc: 0.6321\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 229s 41ms/step - loss: 1.1124 - acc: 0.8054 - val_loss: 1.3859 - val_acc: 0.5920\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 224s 40ms/step - loss: 1.1173 - acc: 0.8010 - val_loss: 1.4223 - val_acc: 0.6047\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 225s 40ms/step - loss: 1.0982 - acc: 0.8146 - val_loss: 1.1998 - val_acc: 0.6871\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 224s 40ms/step - loss: 1.1015 - acc: 0.7971 - val_loss: 0.6676 - val_acc: 0.8668\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 253s 45ms/step - loss: 1.0704 - acc: 0.8137 - val_loss: 1.0945 - val_acc: 0.7167\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 282s 50ms/step - loss: 1.1026 - acc: 0.8097 - val_loss: 0.9163 - val_acc: 0.7421\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 255s 45ms/step - loss: 1.0926 - acc: 0.8206 - val_loss: 0.9997 - val_acc: 0.7336\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 568s 100ms/step - loss: 1.0707 - acc: 0.8197 - val_loss: 1.0556 - val_acc: 0.7230\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 267s 47ms/step - loss: 1.0595 - acc: 0.8173 - val_loss: 0.9394 - val_acc: 0.7865\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 259s 46ms/step - loss: 1.0693 - acc: 0.8185 - val_loss: 0.9499 - val_acc: 0.7653\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 254s 45ms/step - loss: 1.0507 - acc: 0.8227 - val_loss: 0.8225 - val_acc: 0.8436\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 253s 45ms/step - loss: 1.0692 - acc: 0.8229 - val_loss: 0.9955 - val_acc: 0.7442\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 257s 45ms/step - loss: 1.0397 - acc: 0.8293 - val_loss: 1.2307 - val_acc: 0.6808\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 254s 45ms/step - loss: 1.0298 - acc: 0.8261 - val_loss: 0.7055 - val_acc: 0.8668\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 258s 46ms/step - loss: 1.0332 - acc: 0.8261 - val_loss: 0.8307 - val_acc: 0.8097\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 255s 45ms/step - loss: 1.0301 - acc: 0.8288 - val_loss: 1.2217 - val_acc: 0.6829\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 253s 45ms/step - loss: 1.0166 - acc: 0.8279 - val_loss: 0.7858 - val_acc: 0.8309\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 262s 46ms/step - loss: 1.0253 - acc: 0.8227 - val_loss: 0.7019 - val_acc: 0.8562\n",
      "Epoch 2/2\n",
      "5653/5653 [==============================] - 258s 46ms/step - loss: 1.0157 - acc: 0.8330 - val_loss: 0.6982 - val_acc: 0.8879\n",
      "Train on 5653 samples, validate on 473 samples\n",
      "Epoch 1/2\n",
      "5653/5653 [==============================] - 255s 45ms/step - loss: 1.0222 - acc: 0.8298 - val_loss: 0.6906 - val_acc: 0.8753\n",
      "Epoch 2/2\n",
      "1280/5653 [=====>........................] - ETA: 3:10 - loss: 1.0185 - acc: 0.8305"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    x_train, y_train = shuffle_in_unison(x_train, y_train)\n",
    "    for i in range(int(y_train.shape[0]/2)):\n",
    "        x_new, y_new = combine_samples(x_train[i, :, :, :],y_train[i, :], x_train[i+1, :, :, :],y_train[i+1, :])\n",
    "        x_comb[i, :, :, :] = x_new\n",
    "        y_comb[i, :] = y_new\n",
    "    history.append(model.fit(x_comb,\n",
    "                             y_comb,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             callbacks=callbacks,\n",
    "                                 batch_size=256,\n",
    "                             epochs=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately my machine does not allow me to fit the model fast, so the fitting is not complete and a model with a smaller val_loss is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6032206, 0.91120505]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights/BC_d/BC_epoch_002_val_loss_0.6032.hdf5')\n",
    "print(model.test_on_batch(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not understand, how SQ metric works, so i haven't tested this metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
